{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxGDUoANOG0a",
        "outputId": "bf03e286-21d3-4095-ea03-3f96babdb582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model test Score: 0.965,  Model training Score: 0.998\n",
            "Model test Score: 0.993,  Model training Score: 0.995\n",
            "Model test Score: 0.977,  Model training Score: 1.000\n",
            "Model test Score: 0.971,  Model training Score: 1.000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "#iris=load_iris()\n",
        "#X = iris['data']\n",
        "#Y = iris['target']\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "#bc = load_breast_cancer()\n",
        "#X = bc.data\n",
        "#y = bc.target\n",
        "n_estimators = 50\n",
        "\n",
        "#ансамблевая техника - случайный лес\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\"\"\"n_estimators=количество деревьев в лесу. Мы устанавливаем stratify=y для обеспечения того, чтобы и в учебном,\n",
        "и в проверочном наборах данных присутствовало бы то же соотношение 0 и 1, что и в исходном наборе данных.\n",
        "Сначала инициализируем модель. После этого обучим её на масштабированных данных. Точность модели можно измерить на учебных данных.\"\"\"\n",
        "\n",
        "estimators = [\n",
        "     ('rf', RandomForestClassifier(n_estimators, random_state=0)),\n",
        "     ('svr', make_pipeline(StandardScaler(),\n",
        "                           LinearSVC(random_state=0)))]\n",
        "clf = StackingClassifier(\n",
        "    estimators=estimators, final_estimator=LogisticRegression())\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "     X, y, stratify=y, random_state=0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print('Model test Score: %.3f, ' %clf.score(X_test, y_test),\n",
        "      'Model training Score: %.3f' %clf.score(X_train, y_train))\n",
        "\n",
        "#ансамблевая техника - бэггинг\n",
        "\"\"\" max_samples и max_features контролировать размер подмножеств, \n",
        "bootstrap а также bootstrap_features контролировать, будут ли образцы и элементы отображаться с заменой или без нее. \n",
        "При использовании подмножества доступных образцов точность обобщения можно оценить с помощью нестандартных образцов путем настройки oob_score=True. \n",
        "Создаём базовый экземпляр ансамбля KNeighborsClassifier базовых оценок, \n",
        "каждая из которых построена на случайных подмножествах из 50% выборок и 50% функций.\"\"\"\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\"\"\"from sklearn.neighbors import KNeighborsClassifier\n",
        "bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5, n_estimators=100)\n",
        "\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred=bagging.predict(X_test)\n",
        "print('Model test Score: %.3f, ' %bagging.score(X_test, y_test),\n",
        "      'Model training Score: %.3f' %bagging.score(X_train, y_train))\"\"\"\n",
        "\"\"\"Мы используем класс BaggingClassifier для построения модели классификатора пакетов. \n",
        "Здесь мы устанавливаем класс DecisionTreeClassifier в качестве базовой оценки и устанавливаем 100 для количества оценок, \n",
        "а затем обучаем модель\"\"\"\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)\n",
        "dtc = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "bag_model=BaggingClassifier(estimator=dtc, n_estimators=n_estimators, bootstrap=True)\n",
        "bag_model=bag_model.fit(Xtrain,ytrain)\n",
        "print('Model test Score: %.3f, ' %bag_model.score(X_test, y_test),\n",
        "      'Model training Score: %.3f' %bag_model.score(X_train, y_train))\n",
        "\n",
        "#ансамблевая техника - адаптивный бустинг\n",
        "\"\"\"подгоняем последовательность слабых учеников \n",
        "(небольшие деревья решений) на многократно изменяемых версиях данных.\n",
        "Прогнозы от всех из них затем объединяются посредством взвешенного большинства голосов \n",
        "(или суммы) для получения окончательного прогноза. \n",
        "Модификации данных на каждой так называемой итерации повышения состоят в применении весов \n",
        "к каждой из обучающих выборок.\"\"\"\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "modelClf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=2), n_estimators=n_estimators, random_state=0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "modelClf.fit(X_train, y_train)\n",
        "print('Model test Score: %.3f, ' %modelClf.score(X_test, y_test),\n",
        "      'Model training Score: %.3f' %modelClf.score(X_train, y_train))\n",
        "\n",
        "#ансамблевая техника - градиентный бустинг\n",
        "\"\"\"\n",
        "Количество слабых учеников (т. Е. Деревьев регрессии) контролируется параметром n_estimators; \n",
        "Размер каждого дерева можно контролировать либо путем установки глубины дерева через, max_depth\n",
        "либо путем установки количества конечных узлов через max_leaf_nodes. \n",
        "learning_rate гиперпараметр в диапазоне (0,0, 1,0), который контролирует переоснащение посредством усадки.\"\"\"\n",
        "model_Clf = GradientBoostingClassifier(max_depth=2, n_estimators=n_estimators, random_state=0, learning_rate=1)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "model_Clf.fit(X_train, y_train)\n",
        "print('Model test Score: %.3f, ' %model_Clf.score(X_test, y_test),\n",
        "      'Model training Score: %.3f' %model_Clf.score(X_train, y_train))"
      ]
    }
  ]
}